{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notes are result of reading book \"[Python Testing with pytest](https://www.amazon.com/Python-Testing-pytest-Effective-Scalable/dp/1680502409)\" and implementing it on practice.\n",
    "\n",
    "Additional Resources:\n",
    " - https://pythontesting.net/framework/pytest/pytest-introduction/\n",
    " - https://docs.pytest.org/en/latest/index.html\n",
    " - http://hackebrot.github.io/pytest-tricks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* [ ] flask examples\n",
    "* [ ] getting setarted examples\n",
    "* [ ] configuration (`conftest.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T09:58:06.543056Z",
     "start_time": "2019-04-30T09:58:06.496985Z"
    }
   },
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Test Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assert Everything\n",
    "https://wiki.python.org/moin/UsingAssertionsEffectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def test_asserts_ok():\n",
    "    \"\"\" Just Don't: Tuple is True \"\"\"\n",
    "    name = \"ssss\"\n",
    "    assert type(name) is int, (\"name is not a int: {}\".format(name))\n",
    "    \n",
    "def test_asserts_bad():\n",
    "    \"\"\" Just Don't: Tuple is True \"\"\"\n",
    "    name = \"ssss\"\n",
    "    assert (type(name) is int, (\"name is not a int: {}\".format(name))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T06:27:40.144176Z",
     "start_time": "2019-04-30T06:27:40.137974Z"
    }
   },
   "source": [
    "### Assert Exception\n",
    "\n",
    "```python\n",
    "def test_approx():\n",
    "    with pytest.raises(AssertionError):\n",
    "        \"\"\"expected exception\"\"\"\n",
    "        assert .1 + .2 == .3\n",
    "\n",
    "    with pytest.raises(AssertionError):\n",
    "        \"\"\"unexpected exception\"\"\"\n",
    "        assert 4 / 0\n",
    "    \"\"\" using approx \"\"\"\n",
    "    assert .1 + .2 == pytest.approx(.3, rel=1e-19) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups/Teardowns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def setup_module(module):\n",
    "    print(\"\\nAction[{}] / Func[{}] / Argument [{}]\".format(\n",
    "        \"setup_module\", setup_module.__name__, module.__name__))\n",
    "\n",
    "\n",
    "def teardown_module(module):\n",
    "    print(\"\\nAction[{}] / Func[{}] / Argument [{}]\".format(\n",
    "        \"teardown_module\", teardown_module.__name__, module.__name__))\n",
    "\n",
    "\n",
    "def setup_function(func):\n",
    "    print(\"\\nAction[{}] / Func[{}] / Argument [{}]\".format(\n",
    "        \"setup_function\", setup_function.__name__, func.__name__))\n",
    "\n",
    "\n",
    "def teardown_function(func):\n",
    "    print(\"\\nAction[{}] / Func[{}] / Argument [{}]\".format(\n",
    "        \"teardown_function\", teardown_function.__name__, func.__name__))\n",
    "    \n",
    "class TestClass:\n",
    "\n",
    "    @classmethod\n",
    "    def setup_class(cls):\n",
    "        print(\"Class Setup\", cls)\n",
    "\n",
    "    @classmethod\n",
    "    def teardown_class(cls):\n",
    "        print(\"Class teardown\", cls)\n",
    "\n",
    "    def setup_method(self, method):\n",
    "        print(\"Method: Setup, {}\".format(method))\n",
    "\n",
    "    def teardown_method(self, method):\n",
    "        print(\"Method: Teardown, {}\".format(method))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scopes\n",
    "\n",
    "We can user setups and teadowns within scope context (`module`, `function`, `session`, `class`)\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=True)\n",
    "def setup(request):\n",
    "    \"\"\"\n",
    "    request.fixturename\n",
    "    request.scope\n",
    "    https://docs.pytest.org/en/latest/reference.html#request\n",
    "    \"\"\"\n",
    "    print(\"Setup\", request.scope)\n",
    "\n",
    "    def fabrica():\n",
    "        pass\n",
    "\n",
    "    yield fabrica\n",
    "    print(\"TearDown\", request.scope)\n",
    "\n",
    "\n",
    "@pytest.mark.usefixtures(\"setup\")\n",
    "def test_scopes1():\n",
    "    assert True\n",
    "\n",
    "\n",
    "@pytest.mark.usefixtures(\"setup\")\n",
    "def test_scopes2():\n",
    "    assert True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `addfinalizer` to add tearDown steps\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def finalizer_demo(request):\n",
    "    \"\"\" creating some heavy presetup \"\"\"\n",
    "\n",
    "    def teardown1():\n",
    "        \"\"\" destroys pre setup - step 1 \"\"\"\n",
    "        print(\"teardown1\")\n",
    "\n",
    "    request.addfinalizer(teardown1)\n",
    "\n",
    "    def teardown2():\n",
    "        \"\"\" destroys pre setup - step 2 \"\"\"\n",
    "        print(\"teardown2\")\n",
    "\n",
    "    request.addfinalizer(teardown2)\n",
    "\n",
    "\n",
    "def test_finalizer(finalizer_demo):\n",
    "    assert True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skips\n",
    "\n",
    "```python\n",
    "\n",
    "@pytest.mark.skip(reason=\"Lets just skip it\")\n",
    "def test_skipper_1():\n",
    "    \"\"\"skipping it, not going to run\"\"\"\n",
    "    assert False\n",
    "\n",
    "\n",
    "def test_skipper_2():\n",
    "    \"\"\"skipping it, not going to run\"\"\"\n",
    "    pytest.skip(\"skipping it, not going to run\")\n",
    "\n",
    "\n",
    "@pytest.mark.skipif(sys.version_info < (3, 6),\n",
    "                    reason=\"requires python3.6 or higher\")\n",
    "def test_skipper_3():\n",
    "    \"\"\"skipping it, not going to run\"\"\"\n",
    "    print(f\"Running on  Python {sys.version_info[0]}.{sys.version_info[1]}\")\n",
    "\n",
    "\n",
    "py36 = pytest.mark.skipif(sys.version_info < (3, 6),\n",
    "                          reason=\"requires python3.6 or higher\")\n",
    "\n",
    "\n",
    "@py36\n",
    "def test_skipper_4():\n",
    "    \"\"\"skipping it, not going to run\"\"\"\n",
    "    print(f\"Running on  Python {sys.version_info[0]}.{sys.version_info[1]}\")\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-Fail and X-Pass\n",
    "\n",
    "Useful keys \n",
    "\n",
    "* `-rx`  - extra sumarry for : (x)failed\n",
    "* `--lf` - last failed only or all\n",
    "* `--ff` - first failed \n",
    "\n",
    "\n",
    "```python\n",
    "import pytest, sys\n",
    "\n",
    "# Expect to fail, xpass otherwise\n",
    "@pytest.mark.xfail\n",
    "def test_failure1():\n",
    "    assert False\n",
    "\n",
    "# Fail within test function\n",
    "@pytest.mark.xfail\n",
    "def test_failure2():\n",
    "    pytest.fail(msg=\"Failed\", pytrace=True)\n",
    "\n",
    "# Fail if X Pass\n",
    "@pytest.mark.xfail(strict=True)\n",
    "def test_failure3():\n",
    "    assert False\n",
    "\n",
    "# Expect Failure if True \n",
    "@pytest.mark.xfail(sys.version_info < (3, 6), reason=\"python3.6 api changes\")\n",
    "def test_failure4():\n",
    "    answer = 42\n",
    "    assert f\"The answer is {answer}\" == \"The answer is 42\"\n",
    "\n",
    "# Expect Failure if true (expression as string)\n",
    "@pytest.mark.xfail('1==1', reason=\"dummy reason\")\n",
    "def test_failure5():\n",
    "    assert False\n",
    "\n",
    "# fail, don't execute\n",
    "@pytest.mark.xfail(run=False)\n",
    "def test_failure6():\n",
    "    pass\n",
    "\n",
    "# Fail with expection expected - Expections tuple\n",
    "@pytest.mark.xfail(raises=(ZeroDivisionError,))\n",
    "def test_failure7():\n",
    "    4 / 0\n",
    "\n",
    "# Failt with expection expected - Just expection\n",
    "@pytest.mark.xfail(raises=IndexError)\n",
    "def test_failure8():\n",
    "    x = []\n",
    "    x[1] = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builtin Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary Directory\n",
    "\n",
    "```python\n",
    "def test_tmpdir(tmpdir):\n",
    "    CONTENT = \"test\"\n",
    "    p = tmpdir.mkdir(\"sub\").join(\"hello.txt\")\n",
    "    p.write(CONTENT)\n",
    "\n",
    "    assert p.read() == CONTENT\n",
    "    assert len(tmpdir.listdir()) == 1\n",
    "\n",
    "# examples of some methods\n",
    "def pprint(name, what):\n",
    "    base = \"/private/var/folders\"\n",
    "    print(\"{:<10}: {}\".format(name, str(what).replace(base, \"\")))\n",
    "\n",
    "\n",
    "def test_directory(tmpdir):\n",
    "    print()\n",
    "\n",
    "    pprint(\"dir base\", tmpdir)\n",
    "    pprint(\"dir not\", tmpdir / \"n\")\n",
    "\n",
    "    base = tmpdir.mkdir(\"n\")\n",
    "    pprint(\"dir (n)\", base)\n",
    "    base = tmpdir.mkdir(\"f\").mkdir(\"1\")\n",
    "    pprint(\"dir (n1)\", base)\n",
    "    pprint(\"dir (n1:s)\", base.strpath)\n",
    "\n",
    "    aspathfile = base.join(\"hello.txt\")\n",
    "    pprint(\"file\", aspathfile)\n",
    "    pprint(\"file_str\", aspathfile.strpath)\n",
    "\n",
    "    aspathfile.write(\"lol\")\n",
    "\n",
    "    base = tmpdir.mkdir(\"s\").join('inventory')\n",
    "    pprint(\"dir (ensure)\", base.ensure())\n",
    "    pprint(\"dir (join:s)\", base.strpath)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrize\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from collections import OrderedDict\n",
    "\n",
    "xfail = pytest.mark.xfail\n",
    "\n",
    "tests_table = OrderedDict({\n",
    "    'ints': (1, 2, 3),\n",
    "    'floats': (1., 2., 3.),\n",
    "    'sts': (\"twenty\", \"two\", \"twentytwo\"),\n",
    "    'floats_failed':\n",
    "    pytest.param(.1, .2, .3, marks=xfail(reason=\"Float point addition\"))\n",
    "})\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize((\"a\", \"b\", \"expected\"), tests_table.values())\n",
    "def test_sample(a, b, expected):\n",
    "    assert a + b == expected\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"a, b, expected\", tests_table.values(), ids=list(tests_table.keys()))\n",
    "def test_sample2(a, b, expected):\n",
    "    assert a + b == expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functionality\n",
    "\n",
    "| Plugin             | Description  \n",
    "|--------------------|---------------\n",
    "| pytest-repeat      | Run Tests More Than Once\n",
    "| pytest-xdist       | Run Tests in Parallel\n",
    "| pytest-timeout     | Put Time Limits on Your Tests\n",
    "| pytest-instafail   | See Details of Failures and Errors as They Happen\n",
    "| pytest-pycodestyle | Comply with Python’s Style Guide\n",
    "| pytest-pep8        | Comply with Python’s Style Guide\n",
    "| pytest-flake8      | Check for Style + Linting\n",
    "| pytest-pylint      | Check for Style + Linting\n",
    "| pytest-selenium    | Test with a Web Browser\n",
    "| pytest-django      | django\n",
    "| pytest-flask       | flask\n",
    "\n",
    "----\n",
    "\n",
    "```bash\n",
    "# pytest-repeat: repeat all tests twice\n",
    "pytest --count=2\n",
    "# pytest-xdist: automatically detect the number of CPUs \n",
    "pytest -n auto\n",
    "# pytest-timeout: give test .5 seconds to pass\n",
    "pytest --timeout=0.5\n",
    "# pytest-instafail: \n",
    "pytest --instafail --maxfail=2 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations\n",
    "\n",
    "\n",
    "| Plugin             | Description  \n",
    "|--------------------|---------------\n",
    "| pytest-sugar       | Instafail + Colors + Progress Bar\n",
    "| pytest-html        | Generate HTML Reports for Test Sessions\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "```bash\n",
    "# pytest-html: report.html \n",
    "pytest --html=report.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Todo\n",
    "- pytest-runner\n",
    "- pytest-cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure `pytest` in next files `pytest.ini` or `tox.ini` or `setup.cfg`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pytest.ini`\n",
    "\n",
    "```ini\n",
    "[pytest]\n",
    "addopts = -ra --verbose -q\n",
    "testpaths = tests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tox.ini`\n",
    "\n",
    "We can use section `[pytest]` for `pytest` settings.\n",
    "\n",
    "```ini\n",
    "[pytest]\n",
    "addopts = -ra --verbose --cov --cov-report=html\n",
    "testpaths = tests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `setup.cfg`\n",
    "\n",
    "We can use `[tool:pytest]` section to describe settings for pytest.\n",
    "\n",
    "```ini\n",
    "[tool:pytest]\n",
    "addopts = -ra --verbose --cov --cov-report=html\n",
    "testpaths = tests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pytest with other tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `venv` & `pip`\n",
    "\n",
    "Regular virtual environment usage:\n",
    "\n",
    "```bash\n",
    "> python -m venv .venv\n",
    "> source .venv/bin/activate\n",
    "(.venv) > pip install pytest\n",
    "(.venv) > pytest\n",
    "(.venv) > deactivate\n",
    "> \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
