{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8deb0893-c2c6-4c57-8f23-12c0e022bf21",
   "metadata": {},
   "source": [
    "<!-- menu: Testing -->\n",
    "\n",
    "# Testing with `go test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `func TestGo(*testing.T)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f678b1-f556-446e-bc77-7c813c7e9907",
   "metadata": {},
   "source": [
    "### Commands\n",
    "\n",
    "\n",
    "\n",
    "```shell\n",
    "# Coverage for local packages except osme of them...\n",
    "\n",
    "export PKGS=$(go list ./... | grep -vE \"(gotests/gotests|.*data|templates)\" | tr -s '\\n' ',' | sed 's/.\\{1\\}$//')\n",
    "go test -v -covermode=count -coverpkg=$PKGS -coverprofile=coverage.cov\n",
    "\n",
    "# Find what tests were skipped\n",
    "go test -v . | grep SKIP\n",
    "\n",
    "# run tests 10 times + verbose output (also cleans cache)\n",
    "go test -v -test.count 10 .\n",
    "\n",
    "# clean cache\n",
    "go clean -testcache\n",
    "\n",
    "# run on 2 cores\n",
    "go test -v -test.count 10 -test.cpu 2 .\n",
    "\n",
    "# run tests (filter by name)\n",
    "go test -v -run S .\n",
    "\n",
    "# run 4 runners tests\n",
    "go test -v -parallel 4 .\n",
    "\n",
    "# run 4 runners tests\n",
    "go test -race .\n",
    "\n",
    "# json\n",
    "go test -v --json .\n",
    "\n",
    "# just compiling test code\n",
    "go test --exec=/bin/true ./...\n",
    "go test -c pkg\n",
    "\n",
    "# Using jq to filter output of json based export.\n",
    "go test -json | jq -s 'map(select(.Test != null)) | sort_by(.Elapsed)'\n",
    "\n",
    "# benchmarks (+ memory)\n",
    "go test -json -benchmem -run=^$ -bench .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7068481-b6fc-463a-80d5-52a32e5cb53a",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b8b1a-556b-4331-8056-d13b213f8345",
   "metadata": {},
   "source": [
    "#### T(able)DD\n",
    "\n",
    "Add Table Tests or (map) for easy tests examples and adding new code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48768f-1869-4940-8900-85998642556e",
   "metadata": {},
   "source": [
    "#### T(ests)DD\n",
    "\n",
    "TEsts Driven Developments - `first tests then code`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a2420-58c5-4440-828c-c1b2c5d18617",
   "metadata": {},
   "source": [
    "### Linters (`golangci-lint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de985058-d95b-4036-a37b-b7cb5deee1fb",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "- \n",
    "-\n",
    "-\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3a1bd-be06-4374-aa56-42ca73215300",
   "metadata": {},
   "source": [
    "## `func ExampleGo()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc492f8-02c5-4c0a-97f7-b28831d4955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "All examples suppose to be located in `_test.go` files. For easier navigation, make it `file_examples_tests.go` for `file.go`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f0a15-ecbc-4e7f-99a6-3a2598916baf",
   "metadata": {},
   "source": [
    "```go\n",
    "//mygopcg.go\n",
    "package mygopcg\n",
    "\n",
    "type Spanish struct{}\n",
    "\n",
    "// Hello in spanish\n",
    "func (s Spanish) Hello() string {\n",
    "    return \"Hola\"\n",
    "}\n",
    "\n",
    "// PoliteHello tells polite hello\n",
    "func (s Spanish) PoliteHello() string {\n",
    "    return \"Buenos Dias\"\n",
    "}\n",
    "\n",
    "// Greetings return all greetings builtin for Spanish\n",
    "func (s Spanish) Greetings() []string {\n",
    "    var greetings []string\n",
    "    greetings = append(greetings, s.PoliteHello())\n",
    "    greetings = append(greetings, s.Hello())\n",
    "    return greetings\n",
    "}\n",
    "\n",
    "// Hello function does hello!\n",
    "func Hello() string {\n",
    "    return \"Hello\"\n",
    "}\n",
    "```\n",
    "```go\n",
    "//mygopcg_exaamples_test.go\n",
    "package mygopcg\n",
    "\n",
    "import \"fmt\"\n",
    "\n",
    "// Examples can be provided without output, just as example. Such tests wouldn't run.\n",
    "func ExampleHello() {\n",
    "    Hello()\n",
    "}\n",
    "```\n",
    "\n",
    "The naming convention to declare examples for the package, a function `F`, a type `T` and method `M` on type `T` are:\n",
    "\n",
    "- `func Example() { ... }`\n",
    "- `func ExampleF() { ... }`\n",
    "- `func ExampleT() { ... }`\n",
    "- `func ExampleT_M() { ... }`\n",
    "\n",
    "Multiple example functions for a package/type/function/method may be provided by appending a distinct suffix to the name. The suffix must start with a lower-case letter.\n",
    "\n",
    "- `func Example_suffix() { ... }`\n",
    "- `func ExampleF_suffix() { ... }`\n",
    "- `func ExampleT_suffix() { ... }`\n",
    "- `func ExampleT_M_suffix() { ... }`\n",
    "\n",
    "```go\n",
    "// mygopcg_texamples_test.go\n",
    "package mygopcg\n",
    "\n",
    "// In this example you can add `Type` at the end of the example + underscore and method name\n",
    "// to show how your method works for this type.\n",
    "func ExampleSpanish_Hello() {\n",
    "    fmt.Println(Spanish{}.Hello())\n",
    "    // Output: Hola\n",
    "}\n",
    "\n",
    "// By adding custom postfix (starts from underscore and lowercase letter)\n",
    "// you can specify some costom value in braces\n",
    "func ExampleSpanish_Hello_exclamations() {\n",
    "    fmt.Printf(\"¡%s!\\n\", Spanish{}.Hello())\n",
    "    // Output: ¡Hola!\n",
    "}\n",
    "\n",
    "// By adding custom postfix (starts from underscore and lowercase letter)\n",
    "// you can specify some costom value in braces\n",
    "func ExampleSpanish_PoliteHello_polite() {\n",
    "    fmt.Println(Spanish{}.PoliteHello())\n",
    "    // Output: Buenos Dias\n",
    "}\n",
    "\n",
    "// Sometimes if we use maps, we wouldn't get same order. So we shuld use\n",
    "// <code>Unordered output</code> instead Output\n",
    "func ExampleSpanish_Greetings() {\n",
    "    for _, s := range (Spanish{}).Greetings() {\n",
    "    \tfmt.Println(s)\n",
    "    }\n",
    "    // Unordered output: Halo\n",
    "    // Buenos Dias\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce58c0-d70c-4a64-89ca-1049e7a596a9",
   "metadata": {},
   "source": [
    "## `func BenchmarkGo(*testing.B)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bf684-6c02-4e5a-a307-f02c01b443f5",
   "metadata": {},
   "source": [
    "### Reading & Watching & Tooling\n",
    "\n",
    "* [How to write benchmarks in Go](https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go)\n",
    "* [`benchstat`: tips or quickstart](https://github.com/golang/go/issues/23471)\n",
    "* [`benchstat`](https://godoc.org/golang.org/x/perf/cmd/benchstat)\n",
    "* [Go performance measurement, storage, and analysis tools](https://go.googlesource.com/perf/+/83061fdb061356c976cc90806fa391b09da42568#go-performance-measurement_storage_and-analysis-tools)\n",
    "* https://github.com/golang/go/wiki/Performance\n",
    "* [`benchstat` & `pprof` & `benchmarking`](https://www.cloudbees.com/blog/real-life-go-benchmarking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9618844-ed84-4423-b337-7f41d12a1b48",
   "metadata": {},
   "source": [
    "### Arguments \n",
    "\n",
    "* __-bench__ `regexp` - run only benchmarks matching regexp\n",
    "* __-benchmem__ - print memory allocations for benchmarks\n",
    "* __-benchtime__ `d` - run each benchmark for duration d (default `1s`)\n",
    "* __-blockprofile__ `file` - write a goroutine blocking profile to file\n",
    "* __-blockprofilerate__ `rate` - set blocking profile rate (see `runtime.SetBlockProfileRate`) (default `1`)\n",
    "* __-count__ `n` - run tests and benchmarks n times (default 1)\n",
    "* __-coverprofile__ `file` - write a coverage profile to file\n",
    "* __-cpu__ `list` - comma-separated list of cpu counts to run each test with\n",
    "* __-cpuprofile__ `file` - write a cpu profile to file\n",
    "* __-failfast__ - do not start new tests after the first test failure\n",
    "* __-list__ `regexp` - list tests, examples, and benchmarks matching regexp then exit\n",
    "* __-memprofile__ `file` - write an allocation profile to file\n",
    "* __-memprofilerate__ `rate` - set memory allocation profiling rate (see runtime.MemProfileRate)\n",
    "* __-mutexprofile__ `string` - write a mutex contention profile to the named file after execution\n",
    "* __-mutexprofilefraction__ `int` - if >= 0, calls runtime.SetMutexProfileFraction() (default `1`)\n",
    "* __-outputdir__ `dir` - write profiles to dir\n",
    "* __-parallel__ `n` - run at most n tests in parallel (default `24`)\n",
    "* __-run__ `regexp` - run only tests and examples matching regexp\n",
    "* __-short__ - \trun smaller test suite to save time\n",
    "* __-testlogfile__ `file` - \twrite test action log to file (for use only by `cmd/go`)\n",
    "* __-timeout__ `d` - panic test binary after duration d (default `0`, timeout disabled)\n",
    "* __-trace__ `file` - write an execution trace to file\n",
    "* __-v__ - verbose: print additional output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805903b5-7b16-498f-ae17-e84321a8f501",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "> Before concluding I wanted to highlight that to be completely accurate, any benchmark should be careful to avoid compiler optimisations eliminating the function under test and artificially lowering the run time of the benchmark.m\n",
    "\n",
    "```go\n",
    "var result int\n",
    "\n",
    "func BenchmarkFibComplete(b *testing.B) {\n",
    "        var r int\n",
    "        for n := 0; n < b.N; n++ {\n",
    "                // always record the result of Fib to prevent\n",
    "                // the compiler eliminating the function call.\n",
    "                r = Fib(10)\n",
    "        }\n",
    "        // always store the result to a package level variable\n",
    "        // so the compiler cannot eliminate the Benchmark itself.\n",
    "        result = r\n",
    "}\n",
    "\n",
    "\n",
    "// Skipping (not mesuring) time of all not requied things (create reso etc)\n",
    "func bench(b *testing.B, fnc func(...) ...) {\n",
    "    \n",
    "    // b.ResetTimer() // will reset time and memory allocation.\n",
    "                      // doesn't work if timer is running.\n",
    "    \n",
    "\tb.StopTimer()     // <- Stoping timing test\n",
    "    heavyLifting()    // some setup heavylifting  \n",
    "\tb.StartTimer()    // <- Starting timing test\n",
    "\n",
    "    // now we bench\n",
    "\n",
    "\tb.RunParallel(func(pb *testing.PB) {\n",
    "\t\tfor pb.Next() {\n",
    "\t\t\tvar r int\n",
    "\t\t\tfor _, test := range TestCases {\n",
    "\t\t\t\tr = fnc(...)\n",
    "\t\t\t}\n",
    "\t\t\tresult = r\n",
    "\t\t}\n",
    "\t})\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f2e8a-a987-42a1-b928-10f60c77b8f6",
   "metadata": {},
   "source": [
    "```shell\n",
    "# install benchstat                                                           \n",
    "go get -u golang.org/x/perf/cmd/benchstat\n",
    "# running go test with next arguments will run BenchmarkName 10 \n",
    "# times in verbose mode. output will be redirected to results.txt\n",
    "go test ./...  -bench=BenchmarkName -benchmem -run=^$ -count=10 -cpu=1,4  > results.txt\n",
    "\n",
    "# alternativly we can pass regular expression to run all benchmarks \n",
    "go test ./...  -bench='.*' -benchmem -run=^$ -count=10 -cpu=1,4  > results.txt\n",
    "\n",
    "# analyze output\n",
    "benchmark results.txt\n",
    "\n",
    "# more deailed benchmarking with \n",
    "# https://github.com/alecthomas/go_serialization_benchmarks/blob/master/stats.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c827a4-a38f-4bc3-bdf1-6c3e627996d8",
   "metadata": {},
   "source": [
    "### Other \n",
    "\n",
    "#### `b.Run`\n",
    "\n",
    "Sub-Benchmark\n",
    "\n",
    "```golang\n",
    "func Benchmark_...(b *testing.B) {\n",
    "    ....\n",
    "    // we can run subbenchmarks in same way as we running subtests...\n",
    "    b.Run(Name, func(b *testing.B) {\n",
    "        for i := 0; i < b.N; i++ {\n",
    "            // Our testing fucntionality...\n",
    "        }\n",
    "    })\n",
    "}\n",
    "```\n",
    "\n",
    "#### Allocations\n",
    "\n",
    "```golang\n",
    "var result int\n",
    "// Benchmark_...\t 2000000\t       771 ns/op\t     208 B/op\t       7 allocs/op => Go 1.4.2\n",
    "// Benchmark_...  \t 3000000\t       404 ns/op\t     160 B/op\t       3 allocs/op => Go 1.5.0\n",
    "func Benchmark_MoneyNewGetf(b *testing.B) {\n",
    "    va r int\n",
    "\tb.ReportAllocs()\n",
    "\tfor i := 0; i < b.N; i++ {\n",
    "\t\tc := money.New(money.WithPrecision(100)).Setf(-123456.789)\n",
    "\t\tr = c.Getf()\n",
    "\t}\n",
    "    result = r\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7683149-3389-4d31-97e9-93331a5ef17a",
   "metadata": {},
   "source": [
    "## `func FuzzGo(*testing.F)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a309-c175-4828-8d9a-d0874aa11d72",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb41d-d5d1-4e10-90af-0052f5d3ca57",
   "metadata": {},
   "source": [
    "- https://go.dev/doc/tutorial/fuzz\n",
    "- https://go.dev/doc/security/fuzz/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Go",
   "language": "go",
   "name": "go"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": ".go",
   "mimetype": "",
   "name": "go",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "go1.21.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
